{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc6e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5a3eb",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "First, we are going to read the csv, and get a quick overlook on the basic shape of the data etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b87e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ClimateDataBasel.csv', header=None); # There is no headers, will add in next block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e953845f",
   "metadata": {},
   "source": [
    "Below we will define the missing columns from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\n",
    "    \"temp_min\", \n",
    "    \"temp_max\", \n",
    "    \"temp_avg\", \n",
    "    \"humidity_min\", \n",
    "    \"humidity_max\",\n",
    "    \"humidity_avg\", \n",
    "    \"pressure_min\", \n",
    "    \"pressure_max\", \n",
    "    \"pressure_avg\",\n",
    "    \"rain\", \n",
    "    \"snow\", \n",
    "    \"solar\", \n",
    "    \"wind_speed\", \n",
    "    \"wind_dir\",\n",
    "    \"visibility\", \n",
    "    \"air_quality\", \n",
    "    \"ozone\", \n",
    "    \"uv_index\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d1508",
   "metadata": {},
   "source": [
    "Now we are going to get an idea of the general shape of the data, and see if we have any obvious abnormalities. \n",
    "We will also check if there are any features with incorrect data-types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df.columns.tolist()\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcab70e8",
   "metadata": {},
   "source": [
    "Lets next double check if there are any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())  # count missing per column\n",
    "print(df.info())  # check data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ab660",
   "metadata": {},
   "source": [
    "Okay, it doesnt look like there are any missing  values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7845677",
   "metadata": {},
   "source": [
    "Now, lets graph this data to get an idea of the distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ac6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8)) \n",
    "df.hist(bins=30, figsize=(15,10), color='blue', edgecolor='black')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b251e1f",
   "metadata": {},
   "source": [
    "Now lets standardise and normalise our data.\n",
    "In order to do this, we will be using the sci-learn StandardScaler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1233ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler();\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# now to convert back to a dataframe\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "scaled_df.head()\n",
    "\n",
    "\n",
    "\n",
    "# normalise data\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "normaliser = MinMaxScaler(feature_range=(0,1));\n",
    "normalised_data = normaliser.fit_transform(df)\n",
    "normalised_df = pd.DataFrame(normalised_data, columns=df.columns)\n",
    "normalised_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e2347",
   "metadata": {},
   "source": [
    "## Outlier Detection\n",
    "\n",
    "We are going to be using the chebyshev inequality method in order to detect outliers:\n",
    "\n",
    "First, we use k=4, as a strict approach to detect extreme outliers. \n",
    "Then, if needed, we can relax k to 3.\n",
    "my focus is removing the **smallest** amount of data points possible and only removing extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba376fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define k \n",
    "k = 8\n",
    "\n",
    "# get mean and standard deviation\n",
    "mean = df.mean()\n",
    "stdev = df.std()\n",
    "\n",
    "# calculate zscores. These are the number of standard deviations away from the mean a data point lies.\n",
    "z_scores = np.abs((df - mean) / stdev)\n",
    "\n",
    "# if a row has any of these, mark as an outlier.\n",
    "outliers = (z_scores > k).any(axis=1)\n",
    "\n",
    "\n",
    "# Now, we need to display these outliers in a clear way in order to determine if they should be removed.\n",
    "\n",
    "outlier_data = df[outliers]\n",
    "print(f\"Number of outliers detected: {outlier_data.shape[0]}\")\n",
    "print(f\"Outlier ratio: {outlier_data.shape[0] / df.shape[0]:.2%}\")\n",
    "\n",
    "# check if any temp_max values are less than temp_min values\n",
    "temp_anomalies = df[df['temp_max'] < df['temp_min']]\n",
    "print(f\"Number of temperature anomalies detected: {temp_anomalies.shape[0]}\")\n",
    "\n",
    "# check avg temp values are within min and max\n",
    "temp_avg_anomalies =    df[(df['temp_avg'] < df['temp_min'])\n",
    "                        | (df['temp_avg'] > df['temp_max'])]\n",
    "print(f\"Number of temperature average anomalies detected: {temp_avg_anomalies.shape[0]}\")\n",
    "\n",
    "\n",
    "# check if any humidity_max values are less than humidity_min values\n",
    "humidity_anomalies = df[df['humidity_max'] < df['humidity_min']]\n",
    "print(f\"Number of humidity anomalies detected: {humidity_anomalies.shape[0]}\")\n",
    "\n",
    "\n",
    "# check avg humidity values are within min and max\n",
    "humidity_avg_anomalies =    df[(df['humidity_avg'] < df['humidity_min'])\n",
    "                            | (df['humidity_avg'] > df['humidity_max'])]\n",
    "print(f\"Number of humidity average anomalies detected: {humidity_avg_anomalies.shape[0]}\")\n",
    "\n",
    "# check if any pressure_max values are less than pressure_min values\n",
    "pressure_anomalies = df[df['pressure_max'] < df['pressure_min']]\n",
    "print(f\"Number of pressure anomalies detected: {pressure_anomalies.shape[0]}\")\n",
    "\n",
    "# check avg pressure values are within min and max\n",
    "pressure_avg_anomalies =    df[(df['pressure_avg'] < df['pressure_min'])\n",
    "                            | (df['pressure_avg'] > df['pressure_max'])]\n",
    "print(f\"Number of pressure average anomalies detected: {pressure_avg_anomalies.shape[0]}\")\n",
    "\n",
    "outlier_data.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f354ec5",
   "metadata": {},
   "source": [
    "72 Outliers were detected, which is around 4.08% of the total data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79316f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw scatter graphs of all the features, # highlighting the outliers in red\n",
    "# small dots\n",
    "# show them on one image, but a large one\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, column in enumerate(df.columns):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.scatter(df.index, df[column], c=outliers.map({True: 'red', False: 'blue'}), s=10)\n",
    "    plt.title(column)\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(column)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6349b784",
   "metadata": {},
   "source": [
    "After looking through these outliers manually, all of these seem normal. And do not seem like erroneous values that need to be removed. In a weather system especially, outliers or extreme weather events are events of extreme importance and the largest amount of interest. so unless there is a faulty reading, there is no need to remove data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12cc3d",
   "metadata": {},
   "source": [
    "Next, in order to begin clustering this data, we must remove any fields that may confuse the clustering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlaton grid \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create heatmap using matplotlib\n",
    "im = plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(im)\n",
    "\n",
    "# Set ticks and labels\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "\n",
    "# Annotate cells with correlation values\n",
    "for i in range(len(correlation_matrix)):\n",
    "    for j in range(len(correlation_matrix.columns)):\n",
    "        plt.text(j, i, f\"{correlation_matrix.iloc[i, j]:.2f}\",\n",
    "                ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad3206",
   "metadata": {},
   "source": [
    "Now, obviously we can see that the min max and avg variables are redundant for this type of analysis and provide a lot of collinearity. So we shall now do this again with the min and max removed.\n",
    "We may change this if other interesting correlations are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make another correlation grid without min and max ones\n",
    "\n",
    "revised_features = [\n",
    "    \"temp_avg\", \n",
    "    \"humidity_avg\", \n",
    "    \"pressure_avg\", \n",
    "    \"rain\", \n",
    "    \"snow\", \n",
    "    \"solar\", \n",
    "    \"wind_speed\",\n",
    "    \"visibility\", \n",
    "    \"air_quality\", \n",
    "    \"ozone\", \n",
    "    \"uv_index\"\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[revised_features].corr()\n",
    "\n",
    "# Create heatmap using matplotlib\n",
    "im = plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(im)\n",
    "\n",
    "# Set ticks and labels\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "\n",
    "# Annotate cells with correlation values\n",
    "for i in range(len(correlation_matrix)):\n",
    "    for j in range(len(correlation_matrix.columns)):\n",
    "        plt.text(j, i, f\"{correlation_matrix.iloc[i, j]:.2f}\",\n",
    "                ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "plt.title('Correlation Matrix (Revised Features)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7068965a",
   "metadata": {},
   "source": [
    "Now we are going to run pca in order to reduce dimensionality and remove redundancy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "print(\"Explained variance ratio for each component:\", pca.explained_variance_ratio_)\n",
    "print(\"Number of components kept:\", pca.n_components_)\n",
    "\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by Number of Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45683b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pc_columns = [f'PC{i+1}' for i in range(pca.n_components_)]\n",
    "data_pca_df = pd.DataFrame(pca_result, columns=pc_columns)\n",
    "data_pca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000dc36",
   "metadata": {},
   "source": [
    "Now time to run K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k_values = [2, 3, 4, 5, 6, 7]\n",
    "pc1 = data_pca_df['PC1'].values\n",
    "pc2 = data_pca_df['PC2'].values\n",
    "\n",
    "fig, axes = plt.subplots(1, len(k_values), figsize=(20, 4), sharex=True, sharey=True)\n",
    "plt.suptitle('K-Means Clustering Across k Values', fontsize=16, y=1.02)\n",
    "\n",
    "for j, k in enumerate(k_values):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(data_pca_df[pc_columns])  # Use 8 PCs\n",
    "    ax = axes[j]\n",
    "    scatter = ax.scatter(pc1, pc2, c=labels, cmap='viridis', s=2, alpha=1)\n",
    "    ax.set_title(f'k={k}')\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4411055",
   "metadata": {},
   "source": [
    "Now running Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "n_clusters_list = [2, 3, 4, 5]\n",
    "linkage_list = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "fig, axes = plt.subplots(len(linkage_list), len(n_clusters_list), figsize=(16, 12), sharex=True, sharey=True)\n",
    "plt.suptitle('Agglomerative Clustering Across Parameter Grid', fontsize=16, y=1.02)\n",
    "\n",
    "for i, linkage in enumerate(linkage_list):\n",
    "    for j, n_clusters in enumerate(n_clusters_list):\n",
    "        model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "        labels = model.fit_predict(data_pca_df[pc_columns])  # Use all 8 PCs\n",
    "        ax = axes[i, j]\n",
    "        ax.scatter(pc1, pc2, c=labels, cmap='viridis', s=2, alpha=1)\n",
    "        ax.set_title(f'{linkage}, n_clusters={n_clusters}')\n",
    "        ax.set_xlabel('PC1')\n",
    "        ax.set_ylabel('PC2')\n",
    "        ax.label_outer()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911a586",
   "metadata": {},
   "source": [
    "Now running DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "eps_values = [0.3, 0.5, 0.7, 0.9, 1.1]\n",
    "min_samples_values = [3, 5, 8]\n",
    "\n",
    "fig, axes = plt.subplots(len(min_samples_values), len(eps_values), figsize=(18, 10), sharex=True, sharey=True)\n",
    "plt.suptitle('DBSCAN Clustering Across Parameter Grid', fontsize=16, y=1.05)\n",
    "\n",
    "for i, min_samples in enumerate(min_samples_values):\n",
    "    for j, eps in enumerate(eps_values):\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = db.fit_predict(data_pca_df[pc_columns])  # Use all 8 PCs\n",
    "        ax = axes[i, j]\n",
    "        ax.scatter(pc1, pc2, c=labels, cmap='viridis', s=2, alpha=1)\n",
    "        ax.set_title(f'eps={eps}, min_samples={min_samples}\\nclusters={len(set(labels))-(1 if -1 in labels else 0)}\\nnoise={list(labels).count(-1)}')\n",
    "        ax.set_xlabel('PC1')\n",
    "        ax.set_ylabel('PC2')\n",
    "        ax.label_outer()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
